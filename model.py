# -*- coding: utf-8 -*-
"""trainingAndEDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dKkmySXEvw1brm64iX5CCvxCySu_vTfu
"""

import numpy as np

data = np.load("./data/candidate_dataset.npz")
# print(data.files)

x_train = data["x_train"]   # (N, 28, 28)
y_train = data["y_train"]   # (N,)
x_val   = data["x_val"]     # (M, 28, 28)
y_val   = data["y_val"]

# print(x_train.shape, y_train.shape)
# print(x_val.shape, y_val.shape)
# print("Classes:", np.unique(y_train))

# print("Train dtype:", x_train.dtype)
# print("Pixel min/max:", x_train.min(), x_train.max())

x_train = x_train.astype("float32") / 255.0
x_val   = x_val.astype("float32") / 255.0

# print("Train dtype:", x_train.dtype)
# print("Pixel min/max:", x_train.min(), x_train.max())

# print("Train labels:")
# print(" dtype:", y_train.dtype)
# print(" min :", y_train.min())
# print(" max :", y_train.max())
# print(" unique:", np.unique(y_train))

# print("\nValidation labels:")
# print(" dtype:", y_val.dtype)
# print(" min :", y_val.min())
# print(" max :", y_val.max())
# print(" unique:", np.unique(y_val))

y_train = y_train.squeeze()  # (7007,)
y_val   = y_val.squeeze()    # (1003,)

import matplotlib.pyplot as plt

num_classes = 7

train_counts = np.bincount(y_train, minlength=num_classes)
val_counts   = np.bincount(y_val, minlength=num_classes)

# print("Train class counts:", train_counts)
# print("Val class counts:  ", val_counts)

plt.figure()
plt.bar(range(num_classes), train_counts, alpha=0.7, label="Train")
plt.bar(range(num_classes), val_counts, alpha=0.7, label="Validation")
plt.xlabel("Class")
plt.ylabel("Count")
plt.title("Class Distribution: Train vs Validation")
plt.legend()
plt.show()

import random

plt.figure(figsize=(10, 7))

for cls in range(7):
    idxs = np.where(y_train == cls)[0]
    if len(idxs) == 0:
        continue
    i = random.choice(idxs)

    plt.subplot(2, 4, cls + 1)
    plt.imshow(x_train[i], cmap="gray")
    plt.title(f"Class {cls}")
    plt.axis("off")

plt.suptitle("Random Training Sample per Class")
plt.show()

# print(y_val.shape)

import torch
from torch.utils.data import Dataset

class DermatologyDataset(Dataset):
    def __init__(self, images, labels=None):
        """
        images: numpy array (N, 28, 28)
        labels: numpy array (N,) or None for test data
        """
        self.images = images.astype("float32")
        self.labels = labels

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        x = self.images[idx]              # (28, 28, 3)
        x = torch.from_numpy(x).permute(2, 0, 1)  # (3, 28, 28)

        if self.labels is not None:
            y = torch.tensor(self.labels[idx], dtype=torch.long)
            return x, y

        return x

train_ds = DermatologyDataset(x_train, y_train)
val_ds   = DermatologyDataset(x_val, y_val)

# x, y = train_ds[0]
# print(x.shape, y)

from torch.utils.data import DataLoader

batch_size = 64

train_loader = DataLoader(
    train_ds,
    batch_size=batch_size,
    shuffle=True,
    num_workers=0,
    pin_memory=True
)

val_loader = DataLoader(
    val_ds,
    batch_size=batch_size,
    shuffle=False,
    num_workers=0,
    pin_memory=True
)

# xb, yb = next(iter(train_loader))
# print(xb.shape, yb.shape)

import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleCNN(nn.Module):
    def __init__(self, num_classes=7):
        super().__init__()

        self.features = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 14x14

            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 7x7

            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )

        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)

model = SimpleCNN()
# print(model)

# xb, yb = next(iter(train_loader))
# out = model(xb)
# print(out.shape)

criterion = torch.nn.CrossEntropyLoss(
    label_smoothing=0.1,
    reduction="none"   # IMPORTANT
)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# xb, yb = next(iter(train_loader))
# logits = model(xb)
# loss = criterion(logits, yb)

# print("Loss:", loss.item())

def get_keep_ratio(epoch, total_epochs):
    start = 1.0      # keep everything at start
    end = 0.7        # final strictness
    return start - (start - end) * (epoch / total_epochs)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

num_epochs = 10
best_val_acc = 0.0
patience = 3
epochs_no_improve = 0

train_losses = []
val_accuracies = []
keep_ratio = 0.7

for epoch in range(num_epochs):
    # ---- Training ----
    model.train()
    running_loss = 0.0

    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device)

        optimizer.zero_grad()
        logits = model(xb)
        losses = criterion(logits, yb)
        # num_keep = max(1, int(keep_ratio * losses.size(0)))
        current_keep_ratio = get_keep_ratio(epoch, num_epochs)
        num_keep = max(1, int(current_keep_ratio * losses.size(0)))

        sorted_loss, _ = torch.sort(losses)
        # selected_idx = idx[:num_keep]
        filtered_loss = sorted_loss[:num_keep].mean()
        filtered_loss.backward()
        optimizer.step()

        running_loss += filtered_loss.item() * xb.size(0)

    epoch_loss = running_loss / len(train_loader.dataset)
    train_losses.append(epoch_loss)

    # ---- Validation ----
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for xb, yb in val_loader:
            xb, yb = xb.to(device), yb.to(device)
            logits = model(xb)
            preds = torch.argmax(logits, dim=1)
            correct += (preds == yb).sum().item()
            total += yb.size(0)

    val_acc = correct / total
    val_accuracies.append(val_acc)

    print(f"Epoch [{epoch+1}/{num_epochs}] "
          f"Train Loss: {epoch_loss:.4f} "
          f"Val Acc: {val_acc:.4f}")
    print(f"Keep ratio: {current_keep_ratio:.2f}")
    if val_acc > best_val_acc:
      best_val_acc = val_acc
      epochs_no_improve = 0
      torch.save(model.state_dict(), "models/best_model.pth")
      print("✔ Saved best model")
    else:
      epochs_no_improve += 1

    if epochs_no_improve >= patience:
      print("⏹ Early stopping triggered")
      break

import matplotlib.pyplot as plt

plt.figure()
plt.plot(train_losses, label="Train Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss Curve")
plt.legend()
plt.show()

plt.figure()
plt.plot(val_accuracies, label="Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Validation Accuracy Curve")
plt.legend()
plt.show()